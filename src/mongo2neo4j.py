#!/usr/bin/env python3
"""
Imports object structures generated by the ORM mongoose <https://mongoosejs.com/> and
stored in MongoDB <https://www.mongodb.com/> into Neo4j <https://neo4j.com/> for exploration with
SemSpect <https://www.semspect.de/>.
"""

__author__  = 'Marko Luther'
__license__ = 'GPLv3 <https://www.gnu.org/licenses/gpl-3.0.html>'
__version__ = '0.1.0'

# Changelog:
#  v0.1.0 (07/28/2023) : initial version

import sys
import json
import timeit
import argparse
import datetime
from collections.abc import MutableMapping, MutableSequence, Callable, Iterator
from itertools import zip_longest #, takewhile
from bisect import bisect
from typing import Any, TypedDict, NotRequired, TYPE_CHECKING

from pymongo import MongoClient
from bson.objectid import ObjectId
from neo4j import GraphDatabase, Driver, Session
from neo4j.time import DateTime

if TYPE_CHECKING:
    from pymongo.database import Database
    from pymongo.cursor import Cursor


# Types

Item = dict[str, Any]
Items = list[Item]
Link = list[str] # of length 2 holding source and target IDs
Links = list[Link]
Relations = dict[str, Links]
Collections = set[str]
Fields = set[str]
NodeLabels = dict[str, str] # associating node IDs to labels
LabelRelations = dict[str, Relations] # associates labels to relations
SubLabel = tuple[str,str]
SubLabels = list[SubLabel]

class SubSpec(TypedDict):
    'A subtype specification.'
    discriminator: str
    value_type: type # one of {str, list, bool}
    distinct_values: NotRequired[set[str]] # if value_type is bool, distinct_values is not available


# Globals

attr_to_remove: Fields = {
    '__v'  # the document version will always be ignored and never transferred
}



# Utility functions


# see https://stackoverflow.com/questions/24527006/split-a-generator-into-chunks-without-pre-walking-it/34935239#34935239
def chunker(n, iterable):
    '''chunker(3, 'ABCDEFG') --> ('A', 'B', 'C'), ('D', 'E', 'F'),  ('G',)'''
    fillvalue = object()  # Anonymous sentinel object that can't possibly appear in input
    arguments = (iter(iterable),) * n
    for x in zip_longest(*arguments, fillvalue=fillvalue):
        if x[-1] is fillvalue:
            # takewhile optimizes a bit for when n is large and the final
            # group is small; at the cost of a little performance, you can
            # avoid the takewhile import and simplify to:
            # yield tuple(v for v in x if v is not fillvalue)
#            yield tuple(takewhile(lambda v: v is not fillvalue, x))
            yield x[:bisect(x, False, key=lambda v: v is fillvalue)]  # Python 3.10+ only!
        else:
            yield x


# see https://stackoverflow.com/questions/54815892/pymongo-cursor-batch-size
def yield_rows(cursor: 'Cursor[Any]', chunk_size: int) -> Iterator[Items]:
    """
    Generator to yield chunks from cursor
    :param cursor:
    :param chunk_size:
    :return:
    """
    chunk: Items = []
    for i, row in enumerate(cursor):
        if i % chunk_size == 0 and i > 0:
            yield chunk
            del chunk[:]
        chunk.append(row)
    yield chunk


def apply_recursive(func: Callable[[Any], Any], obj: Any) -> Any:
    """Applies the given function recursively to all elements of the give object."""
    if isinstance(obj, dict):  # if dict, apply to each key
        return {k: apply_recursive(func, v) for k, v in obj.items()}
    if isinstance(obj, list):  # if of type list, apply to each element
        return [apply_recursive(func, elem) for elem in obj]
    return func(obj)


# adds the source-target tuples of new_links under rel to relations
def add_relation(relations: Relations, rel: str, new_link: Link) -> None:
    """Adds destructively the source-target tuple of new link under key rel to the given relations map"""
    if rel in relations:
        relations[rel].append(new_link)
    else:
        relations[rel] = [new_link]


# adds the source-target tuples of new_links under rel to relations
def add_relations(relations: Relations, rel: str, new_links: Links) -> None:
    """Adds destructively the source-target tuples of new links under key rel to the given relations map"""
    if rel in relations:
        relations[rel].extend(new_links)
    else:
        relations[rel] = new_links


def update_relations(relations: Relations, new_relations: Relations) -> None:
    """Adds destructively the new_relations to the given relations map"""
    for (key, values) in new_relations.items():
        add_relations(relations, key, values)


# converts ObjectId(_) to string
def flatten_and_cleanse(
        dictionary: MutableMapping[str, Any],
        parent_key: str = '',
        node_id: str = '',
        separator: str = '_',
        suppress: None | Fields = None) -> tuple[Item, Relations, Fields]:
    """Returns the given dictionary with sub dictionaries flattened,
        objectId(_) converted to strings and
        keys in the 'suppress' list removed.
        As second argument a dictionary associating relation names
        with list of tuples of source and target node ids.
        As third acgument a set of attributes of type list."""
    if suppress is None:
        suppress = set()
    res: Item = {}
    relations: Relations = {}
    array_fields: Fields = set()

    if parent_key == '' and '_id' in dictionary:
        node_id = str(dictionary['_id'])

    for key, value in dictionary.items():
        if key not in suppress and value is not None and value != '' and value != []:
            new_key = parent_key + separator + key if parent_key else key
            if isinstance(value, MutableMapping):
                rec_res, rec_relations, rec_array_fields = flatten_and_cleanse(value, new_key, node_id, separator, suppress)
                res.update(rec_res)
                update_relations(relations, rec_relations)
                array_fields.update(rec_array_fields)
            elif isinstance(value, ObjectId):
                id_str:str = str(value)
                if new_key != '_id' and node_id != '':
                    add_relation(relations, new_key, [node_id, id_str])
                else:
                    res[new_key] = id_str
            elif isinstance(value, MutableSequence):
                if all(isinstance(v, str) for v in value):
                    # homogeneous list of numbers or strings
                    res[new_key] = value
                    array_fields.add(new_key) # mark as field of type array
                elif (all(isinstance(v, (int)) for v in value) or
                        all(isinstance(v, (float)) for v in value)):
                    # homogeneous list of numbers (not marked as field of type array as field is ignored, but links are generated)
                    res[new_key] = value
                elif all(isinstance(v, ObjectId) for v in value) and new_key != '_id' and node_id != '':
                    # homogeneous list of ObjectIds
                    # we discard the field, but establish the relations
                    add_relations(relations, new_key, [[node_id, str(v)] for v in value])
            else:
                res[new_key] = value
    return res, relations, array_fields


def split_attributes(dictionary: Item) -> tuple[Item, Item]:
    """Splits the given attributes into those with primitive values and those with \
        non-primitive ones"""
    primitive: Item = {}
    non_primitive: Item = {}
    for key, value in dictionary.items():
        if isinstance(value, list) and not all(
                isinstance(e, (str | bool | int | float)) for e in value):
            non_primitive[key] = value
        else:
            primitive[key] = value
    return primitive, non_primitive



# Neo4j communication

def render_value(value: Any) -> str:
    """Render given value as str"""
    if isinstance(value, DateTime):
        return f"datetime('{value}')"
    if isinstance(value, datetime.datetime):
        value_as_isoformat_datetime: str = value.isoformat(sep='T')
        return f"datetime('{value_as_isoformat_datetime}')"
    if isinstance(value, bool):
        return str(value).lower()
    if isinstance(value, (int | float)):
        return str(value)
    return f"'{value}'"


def neo4j_output_query(query, **kwargs) -> None:
    """Outputs the given Cypher query to stdout."""
    result = query
    for key, value in kwargs.items():
        result = result.replace(
            f'${key}', render_value(value))
    print(result)


def neo4j_run_read_query(
        session: None | Session, verbose: bool, query, **kwargs) -> None:
    """Sends the given query to Neo4j and outputs it to stdout if verbose is True."""

    def work(tx):  # pylint: disable=invalid-name
        return tx.run(query, **kwargs)

    if verbose:
        neo4j_output_query(query, **kwargs)
    if session is not None:
        session.execute_read(work)


def neo4j_run_write_query(
        session: None | Session, verbose: bool, query, **kwargs) -> None:
    """Sends the given query to Neo4j and outputs it to stdout if verbose is True."""

    def work(tx):  # pylint: disable=invalid-name
        return tx.run(query, **kwargs)

    if verbose:
        neo4j_output_query(query, **kwargs)
    if session is not None:
        session.execute_write(work)


def neo4j_create_index(session: None | Session, verbose: bool, label: str, field: str) -> None:
    """Creates an index on field <field> of <label>"""
    neo4j_run_write_query(
        session,
        verbose,
        f'CREATE INDEX {label.replace(" ", "_")}_{field.replace(" ", "_")} IF NOT EXISTS FOR (l:`{label}`) ON l.`{field}`')


def neo4j_add_sublabel(
        session: None | Session,
        verbose: bool,
        chunk_size: int,
        apoc_installed: bool,
        label: str,
        field: str,
        value: str,
        sublabels: list[str],
        isarray: bool = False) -> None:
    """Adds <sublabel> to nodes in <label> where <value> is in list of <field> field values if isarray is True,
       or if isarray is False and <field> value equals <value>."""
    if len(sublabels) > 0:
        if isarray:
            match_clause = f'MATCH (n:`{label}`) WHERE {value} IN n.`{field}`'
        else:
            match_clause = f'MATCH (n:`{label}` {{`{field}`: {value}}})'
        quoted_sublabels = [f'`{s}`' for s in sublabels]
        set_clause = f'SET n:{":".join(quoted_sublabels)}'
        if apoc_installed:
            neo4j_run_write_query(
                session,
                verbose,
                (f'CALL apoc.periodic.iterate("{match_clause} RETURN n",'
                 f'"{set_clause}",'
                 f'{{batchSize:{chunk_size}, parallel:true}})'))
        else:
            neo4j_run_write_query(
                session,
                verbose,
                f'{match_clause} {set_clause}')


# pylint: disable=too-many-arguments
def process_data(
        database: 'Database',
        session: None | Session,
        collections: Collections,
        excluded_collections: Collections,
        excluded_fields: Fields,
        sublabels: SubLabels,
        create: bool,
        verbose: bool,
        chunk_size: int) -> None:
    """Transfers all <collections> of the given MongoDB <db> via the <session> to Neo4j's \
        <neo4j_db> DB excluding the <excluded_collections> and the <excluded_fields>.
        If verbose, the generated Cypher queries are printed to stdout."""

    node_label: NodeLabels = {} # associates the unique node _id to its label on creation of a node
    all_relations: LabelRelations = {} # associates labels with relations given as association of relation names with list of tuples of source and target node ids
    active_collections:Collections = collections.difference(excluded_collections)
    array_fields = set()

    relations: Relations
    collection: str


    # add nodes

    for collection in active_collections:
        item_count:int = database[collection].count_documents({})
        if item_count > 0:
            print(f"*** processing nodes of collection '{collection}'")
            relations = {}
            cursor: Cursor[Any] = database[collection].find({}, batch_size=chunk_size)
            chunk: Items

            # Establish constraints on labels
            # NOTE: the use of $ query parameters for constraint name and label
            #       does not work here, so we use Python fstring substitution
            neo4j_run_write_query(
                session,
                verbose,
                (f'CREATE CONSTRAINT `{collection}_cstr` IF NOT EXISTS FOR (l:`{collection}`) '
                 'REQUIRE l._id IS UNIQUE'))

            for chunk in yield_rows(cursor, chunk_size):
                cleansed_data:Items = []
                # first we convert all objects in per chunk and extract its relations
                for obj in chunk:
                    obj_data, obj_relations, obj_array_fields = flatten_and_cleanse(obj, suppress=attr_to_remove.union(excluded_fields))
                    node_id: str = obj_data['_id']
                    if node_id in node_label:
                        print(f"ignoring object with duplicate ObjectId '{node_id}':{collection} (already registered as with label '{node_label[node_id]}').")
                    else:
                        cleansed_data.append(obj_data)
                        update_relations(relations, obj_relations)
                        node_label[node_id] = collection
                        array_fields.update(obj_array_fields)
                # create the nodes per chunk
                data: dict[str, Items] = { 'batch': cleansed_data }
                query = f"UNWIND $batch as row {('CREATE' if create else 'MERGE')} (n:`{collection}` {{_id: row._id}}) SET n += row"
                neo4j_run_write_query(session, verbose, query, **data)
                sys.stdout.write('.')
                sys.stdout.flush()
            print('') # add a newline
            all_relations[collection] = relations


    # add relations
    for collection, relations in all_relations.items():
        print(f"*** processing relations of collection '{collection}'")

        # create relations (also per chunk)
        for rel, all_links in relations.items():
            links = [l for l in all_links if l[0] in node_label and l[1] in node_label] # only create relations between labeled nodes
            print(rel, len(links))
            if len(links) > 0 and len(links[0])>1:
                target_label:str = node_label[links[0][1]]
                for links_chunk in chunker(chunk_size, links):
                    data = { 'links': links_chunk }
                    query = (
                        f'UNWIND $links as row MATCH (a:`{collection}`), (b:`{target_label}`) '
                        f"WHERE a._id = row[0] and b._id = row[1] {('CREATE' if create else 'MERGE')} (a)-[:{rel}]->(b)")
                    neo4j_run_write_query(session, verbose, query, **data)
                    sys.stdout.write('.')
                    sys.stdout.flush()
                print('') # add a newline


    # add sublabels

    # we only generate sublabels for active collections
    sublabels = [sl for sl in sublabels if sl[0] in active_collections]
    if sublabels:
        print('*** processing sublabels')
        established_sublabels: set[str] = set()
        apoc_installed:bool = False
        if session is not None:
            try:
                neo4j_run_read_query(
                    session,
                    verbose,
                    'RETURN apoc.version() AS output')
                apoc_installed = True
                print('APOC installed')
            except Exception:
                # apoc plugin not installed
                pass
        sub_label_specs:dict[str, list[SubSpec]] = {}
        # extract distinct values and type per sublabel item and associate it with label in sub_label_spec
        for (label, discriminator) in sublabels:
            # distinct values, without None and the empty string
            distinct_values:set[Any] = set(database[label].distinct(discriminator)).difference({None, ''})
            sub_label_spec_item: None|SubSpec = None
            if (all(isinstance(item, bool) for item in distinct_values) and
                    True in distinct_values and
                    discriminator not in active_collections and
                    discriminator not in established_sublabels):
                # we use only the discriminator as sublabel for boolean typed discriminators
                sub_label_spec_item = SubSpec(
                    discriminator = discriminator,
                    value_type = bool)
                # remember newly created (sub-)label
                established_sublabels.add(discriminator)
            elif (1 < len(distinct_values) < 50 and
                    all(isinstance(item, str) for item in distinct_values) and
                    not active_collections.intersection(distinct_values) and
                    not established_sublabels.intersection(distinct_values)):
                sub_label_spec_item = SubSpec(
                    discriminator = discriminator,
                    value_type = (list if (discriminator in array_fields) else str),
                    distinct_values = distinct_values)
                # remember newly created (sub-)label
                new_sublabels:list[str] = [item for sublist in [sl.split('#') for sl in distinct_values] for item in sublist]
                new_sublabels.append(discriminator)
                established_sublabels.update(new_sublabels)
            if sub_label_spec_item is not None:
                if label in sub_label_specs:
                    sub_label_specs[label].append(sub_label_spec_item)
                else:
                    sub_label_specs[label] = [sub_label_spec_item]

        for label, sub_specs in sub_label_specs.items():
            multiple_sublabels:bool = len([ss for ss in sub_specs if ss['value_type'] != bool])>1
            for sub_spec in sub_specs:
                if sub_spec['value_type'] is bool:
                    discriminator = sub_spec['discriminator']
                    neo4j_create_index(session, verbose, label, discriminator)
                    neo4j_add_sublabel(session, verbose, chunk_size, apoc_installed, label, discriminator, 'true', [discriminator])
                    print(':')
                elif sub_spec['value_type'] in [str, list]:
                    # first establish index on the discriminator
                    isarray:bool = sub_spec['value_type'] is list
                    discriminator = sub_spec['discriminator']
                    if not isarray:
                        # for fields of type list the index is not effective
                        neo4j_create_index(session, verbose, label, discriminator)
                    for sl in sub_spec['distinct_values']:
                        # add sublabels
                        labels: list[str] = sl.split('#')
                        if multiple_sublabels:
                            labels.append(discriminator)
                        neo4j_add_sublabel(session, verbose, chunk_size, apoc_installed, label, discriminator, f"'{sl}'", labels, isarray)
                        sys.stdout.write('.')
                        sys.stdout.flush()
                    print('') # add a newline


# pylint: disable=too-many-locals
def main(
        mongo_host: str,
        mongo_port: int,
        mongo_db: str,
        neo4j_host: str,
        neo4j_port: int,
        neo4j_user: str,
        neo4j_password: str,
        neo4j_db: str,
        chunk_size: int,
        excluded_collections: Collections,
        included_collections: None | Collections,
        excluded_fields: Fields,
        sublabels: SubLabels,
        create: bool = False,
        verbose: bool = False,
        simulate: bool = False) -> None:
    """The main mongo2neo4j function that takes MongoDB credential and DB name, Neo4j credentials \
        and DB name and a specification which MongoDB collections and attributes should be \
        transferred to Neo4j, if the Cypher queries should be printed to stdout (verbose=True) \
        and if the Neo4j engine should be connected to or not (simulate=True)."""

    # connect to MongDB
    mongo_client: MongoClient = MongoClient(mongo_host, mongo_port)
    # get MongoDB DB
    database: 'Database' = mongo_client[mongo_db]

    db_collections: list[str] = database.list_collection_names()
    collections: Collections = (
        set(db_collections)
        if included_collections is None
        else set(included_collections).intersection(db_collections))

    if simulate:
        process_data(
            database,
            None,
            collections,
            excluded_collections,
            excluded_fields,
            sublabels,
            create,
            verbose,
            chunk_size)
    else:
        driver: Driver
        with GraphDatabase.driver(  # pylint: disable=not-context-manager
                f'neo4j://{neo4j_host}:{neo4j_port}',
                auth=(neo4j_user, neo4j_password)) as driver:
            session: Session
            with driver.session(database=neo4j_db) as session:
                process_data(
                    database,
                    session,
                    collections,
                    excluded_collections,
                    excluded_fields,
                    sublabels,
                    create,
                    verbose,
                    chunk_size)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        '--conf_export',
        action='store_true',
        help='Dump config and exit')
    parser.add_argument(
        '--conf',
        action='append',
        default=['mongo2neo4j.conf'],
        help='Read config file')
    parser.add_argument('-v', '--verbose', action='store_true', help='Output Cypher')
    parser.add_argument(
        '-s', '--simulate', action='store_true', help="Don't connect to Neo4j")
    parser.add_argument(
        '-c',
        '--create',
        action='store_true',
        help='Use CREATE instead of MERGE to create objects in Neo4j')
    parser.add_argument(
        '-mh',
        '--mongo_host',
        dest='mongo_host',
        type=str,
        default='localhost',
        help='MongoDB hostname')
    parser.add_argument(
        '-mp',
        '--mongo_port',
        dest='mongo_port',
        type=int,
        default='27017',
        help='MongoDB port')
    parser.add_argument(
        '-nh',
        '--neo4j_host',
        dest='neo4j_host',
        type=str,
        default='localhost',
        help='Neo4j hostname')
    parser.add_argument(
        '-np',
        '--neo4j_port',
        dest='neo4j_port',
        type=int,
        default='7687',
        help='Neo4j port')
    parser.add_argument(
        '-nu',
        '--neo4j_user',
        dest='neo4j_user',
        type=str,
        default='neo4j',
        help='Neo4j user')
    parser.add_argument(
        '-npw',
        '--neo4j_password',
        dest='neo4j_password',
        type=str,
        required=True,
        help='Neo4j password')
    parser.add_argument(
        '-nd', '--neo4j_db',
        dest='neo4j_db',
        type=str,
        help='Neo4j DB to import into')
    parser.add_argument(
        '-k',
        '--chunk_size',
        dest='chunk_size',
        type=int,
        default='500',
        help='processing objects chunk size')
    parser.add_argument(
        '-i',
        '--include',
        dest='included_collections',
        action='append',
        help='Collection to be transferred. If not specified, transfer all not excluded ones.')
    parser.add_argument(
        '-x',
        '--exclude',
        dest='excluded_collections',
        action='append',
        default=[],
        help='Collection to be excluded')
    parser.add_argument(
        '-f',
        '--exclude_fields',
        dest='excluded_fields',
        action='append',
        default=[],
        help='MongoDB document fields to be ignored')
    parser.add_argument(
        '-sl',
        '--sublabels',
        dest='sublabels',
        action='append',
        default=[],
        help='List of <Collection>.<attrib> of type string or list of strs to create sublabels')
    parser.add_argument('mongo_db', help='MongoDB DB to be imported')
    args = parser.parse_args()

    # load config file
    if args.conf is not None:
        for conf_fname in args.conf:
            with open(conf_fname, encoding='utf-8') as f:
                parser.set_defaults(**json.load(f))
    # Reload arguments to override config file values with command line values
    args = parser.parse_args()

    # dump config
    if args.conf_export:
        tmp_args = vars(args).copy()
        del tmp_args['conf_export']  # Do not dump value of conf_export flag
        del tmp_args['conf']  # Values already loaded
        print(json.dumps(tmp_args,  indent=4, sort_keys=True))
        sys.exit(-1)  # Optional, if system should on print config and exit

    requested_collections: None | Collections = None
    if args.included_collections is not None:
        requested_collections = set()
        for c in args.included_collections:
            for s in c.split(','):
                requested_collections.add(s)
    args_excluded_collections: Collections = set()
    for c in args.excluded_collections:
        for s in c.split(','):
            args_excluded_collections.add(s)
    args_excluded_fields: Fields = set()
    if args.excluded_fields is not None:
        for fields in args.excluded_fields:
            for f in fields.split(','):
                args_excluded_fields.add(str(f))

    print(timeit.timeit(number=1, stmt=lambda:
        main(
            args.mongo_host,
            args.mongo_port,
            args.mongo_db,
            args.neo4j_host,
            args.neo4j_port,
            args.neo4j_user,
            args.neo4j_password,
            args.neo4j_db,
            args.chunk_size,
            args_excluded_collections,
            requested_collections,
            args_excluded_fields,
            [(*sl.split('.')[:2],) for sl in args.sublabels if '.' in sl], # type: ignore [misc]
            args.create,
            args.verbose,
            args.simulate)))
